<?xml version="1.0" encoding="utf-8"?>
<AxClass xmlns:i="http://www.w3.org/2001/XMLSchema-instance">
	<Name>TaxGSTRReportMergeFile</Name>
	<SourceCode>
		<Declaration><![CDATA[
using Microsoft.DynamicsOnline.Infrastructure.Components.SharedServiceUnitStorage;
/// <summary>
/// The class for GSTR report multiple batch merge file.
/// </summary>
public class TaxGSTRReportMergeFile extends RunBaseBatch implements BatchRetryable
{
    Description nameStr;
    RefRecId taxBatchJobGroup;
    ERFormatMappingRunJobTable runJobTable;
    System.IO.MemoryStream zipArchiveStream;

    #define.CurrentVersion(1)

    #LOCALMACRO.CurrentList
        nameStr,
        taxBatchJobGroup
    #ENDMACRO

}
]]></Declaration>
		<Methods>
			<Method>
				<Name>pack</Name>
				<Source><![CDATA[
    /// <summary>
    /// Packs the object, used to send the object to batch.
    /// </summary>
    /// <returns>A packed Incremental Changes History Purge Job</returns>
    public container pack()
    {
        return [#CurrentVersion, #CurrentList];
    }

]]></Source>
			</Method>
			<Method>
				<Name>unpack</Name>
				<Source><![CDATA[
    /// <summary>
    /// Unpacks the object, used by batch to obtain object parameters
    /// </summary>
    /// <param name = "packedClass">A packed Incremental Changes History Purge Job object</param>
    /// <returns>True if success, false otherwise.</returns>
    public boolean unpack(container packedClass)
    {
        Integer version = conPeek(packedClass, 1);

        switch (version)
        {
            case #CurrentVersion:
                [version,#CurrentList] = packedClass;
                break;
            default:
                return false;
        }
        return true;
    }

]]></Source>
			</Method>
			<Method>
				<Name>canGoBatchJournal</Name>
				<Source><![CDATA[
    /// <summary>
    /// Determines whether the class is shown in the list of <c>Journal</c> types.
    /// </summary>
    /// <returns>
    /// true if the class is shown in the list of <c>Journal</c> types; otherwise, false.
    /// </returns>
    /// <remarks>
    /// A class that can be used in a batch journal is a class where the same parameters can be used
    /// repeatedly. The dialog box can be shown and the parameters can be changed but parameters of some
    /// classes might build on data that is only valid for a short time. Running a class two times with the
    /// same parameters is not always possible. If <see cref="M:RunBaseBatch.canGoBatch" /> is false, this
    /// method will not have any effect.
    /// </remarks>
    public boolean canGoBatchJournal()
    {
        return true;
    }

]]></Source>
			</Method>
			<Method>
				<Name>runsImpersonated</Name>
				<Source><![CDATA[
    /// <summary>
    /// Indicates whether the batch job will be run with the account of the user who created the job.
    /// </summary>
    /// <returns>
    /// Always returns true.
    /// </returns>
    /// <remarks>
    /// If you want a batch job that extends this class to run on a client, you must override this method
    /// to return false. Jobs that return true must not access any client-side functionality. If true is
    /// returned, the batch server will execute the job by using the <c>runAs</c> method.
    /// </remarks>
    public boolean runsImpersonated()
    {
        return true;
    }

]]></Source>
			</Method>
			<Method>
				<Name>canRunInNewSession</Name>
				<Source><![CDATA[
    /// <summary>
    /// Run in new session.
    /// </summary>
    protected boolean canRunInNewSession()
    {
        return false;
    }

]]></Source>
			</Method>
			<Method>
				<Name>run</Name>
				<Source><![CDATA[
    public void run()
    {

        runJobTable.clear();
        runJobTable.Caption = nameStr;
        runJobTable.Status = ERFormatMappingRunJobStatus::Waiting;
        runJobTable.BatchJob = this.parmCurrentBatch().BatchJobId;
        runJobTable.insert();
        
        TaxBatchJobGroup taxBatchJobGroupLoc;
        taxBatchJobGroupLoc = TaxBatchJobGroup::find(taxBatchJobGroup);

        boolean allBatchJobFinished;
        while (!allBatchJobFinished)
        {
            sleep(100);
            allBatchJobFinished = taxBatchJobGroupLoc.isAllBatchJobFinished();
        }

        TaxBatchJobRelation taxBatchJobRelation;
        select firstonly taxBatchJobRelation
            where taxBatchJobRelation.TaxBatchJobGroup == taxBatchJobGroup
            && taxBatchJobRelation.IsMergeBatch == NoYes::No;

        this.attachMergeFile(ERFormatMappingRunJobTable::find(taxBatchJobRelation.ERFormatMappingRunJobTable));
        
        taxBatchJobGroupLoc.finish(); 

        this.addTelemetry(taxBatchJobGroupLoc);
    }

]]></Source>
			</Method>
			<Method>
				<Name>parmName</Name>
				<Source><![CDATA[
    [Hookable(false)]
    public Description parmName(Description _name = nameStr)
    {
        nameStr  = _name;
        return nameStr;
    }

]]></Source>
			</Method>
			<Method>
				<Name>parmTaxBatchJobGroup</Name>
				<Source><![CDATA[
    [Hookable(false)]
    public RefRecId parmTaxBatchJobGroup(RefRecId _recId = taxBatchJobGroup)
    {
        taxBatchJobGroup = _recId;
        return taxBatchJobGroup;
    }

]]></Source>
			</Method>
			<Method>
				<Name>mergeFileContent</Name>
				<Source><![CDATA[
    /// <summary>
    /// Merge file content.
    /// </summary>
    /// <param name = "_recId">file RefRecId mark</param>
    /// <returns>return file name and content</returns>
    protected container mergeFileContent(RefRecId _recId)
    {
        ERFormatMappingRunJobTable          jobLoc;
        TaxBatchJobRelation  jobTable;
        container fileName, fileContent;
        int num = 0;

        while select jobLoc
            join ERFormatMappingRunJobTable from jobTable
                where jobTable.ERFormatMappingRunJobTable == jobLoc.RecId
                    && jobTable.TaxBatchJobGroup == _recId
        {
            DocuRef docuRef = DocuRef::findTableIdRecId(jobLoc.DataAreaId, jobLoc.TableId, jobLoc.RecId);
            System.IO.Compression.ZipArchive  zipArchive = this.getZipArchiveByDocuRef(docuRef);
            CLRObject archiveEntries =  zipArchive.get_Entries();
            int length = archiveEntries.get_Count();
            for(int i = 0; i < length; i++)
            {
                System.IO.Compression.ZipArchiveEntry entry = archiveEntries.get_Item(i);
                if (num == 0)
                {
                    fileName += [entry.Name];
                }
                System.IO.StreamReader read = new System.IO.StreamReader(entry.Open());
                if (num == 0)
                {
                    System.String string = read.ReadToEnd();
                    fileContent += [string];
                }
                else
                {
                    read.ReadLine();
                    System.String string = read.ReadToEnd();
                    if (!System.String::IsNullOrEmpty(string))
                    {
                        fileContent = conPoke(fileContent, i + 1, conPeek(fileContent, i + 1) + '\n' + string);
                    }
                }
            }
            num++;
        }

        return [fileName, fileContent];
    }

]]></Source>
			</Method>
			<Method>
				<Name>mergeFile</Name>
				<Source><![CDATA[
    /// <summary>
    /// Merge file.
    /// </summary>
    /// <param name = "fileName">file name</param>
    /// <param name = "fileContent">file content</param>
    private void mergeFile(container fileName, container fileContent)
    {
        zipArchiveStream = new System.IO.MemoryStream();
        using (System.IO.Compression.ZipArchive zipArchive = new System.IO.Compression.ZipArchive(zipArchiveStream, System.IO.Compression.ZipArchiveMode::Create, true))
        {
            for (int i = 1; i <= conLen(fileName); i++)
            {
                System.Text.Encoding encoding = System.Text.Encoding::get_UTF8();
                System.String string = conPeek(fileContent, i);
                System.Byte[] byteArray = encoding.GetBytes(string);
                System.IO.MemoryStream mestream = new System.IO.MemoryStream(byteArray);
                System.IO.Compression.ZipArchiveEntry entry = zipArchive.CreateEntry(conPeek(fileName, i));
                using (System.IO.Stream stream = entry.Open())
                {
                    mestream.CopyTo(stream);
                }
            }
        }

        zipArchiveStream.Seek(0, System.IO.SeekOrigin::Begin);
    }

]]></Source>
			</Method>
			<Method>
				<Name>attachMergeFile</Name>
				<Source><![CDATA[
    private void attachMergeFile(ERFormatMappingRunJobTable _job)
    {
        ttsbegin;
        runJobTable.selectForUpdate(true);

        container fileName, fileContent;

        [fileName, fileContent] = this.mergeFileContent(taxBatchJobGroup);
        this.mergeFile(fileName, fileContent);

        Filename fileNameStr = DocuRef::findTableIdRecId(_job.DataAreaId, _job.TableId, _job.RecId).docuValue().fileName();

        DocumentManagement::attachFile(runJobTable.TableId, runJobTable.RecId, runJobTable.DataAreaId, DocuType::typeFile(), zipArchiveStream, fileNameStr, System.Web.MimeMapping::GetMimeMapping(fileNameStr), fileNameStr);

        runJobTable.Status = ERFormatMappingRunJobStatus::Finished;
        runJobTable.update();
        ttscommit;
    }

]]></Source>
			</Method>
			<Method>
				<Name>getZipArchiveByDocuRef</Name>
				<Source><![CDATA[
    private System.IO.Compression.ZipArchive getZipArchiveByDocuRef(DocuRef docuRef)
    {
        DocuValue docuValue = docuRef.docuValue();
        str uniqueFileName = strfmt("%1/%2", docuValue.FileId, docuValue.fileName());
        FileUploadTemporaryStorageStrategy strategy = new FileUploadTemporaryStorageStrategy();
        var blobStorageMetadata = new SharedServiceUnitStorageData();
        try
        {
            blobStorageMetadata.Id = guid2Str(docuValue.FileId);
            blobStorageMetadata.Category = strategy.getAzureStorageCategory();
            blobStorageMetadata.Name = uniqueFileName;
            blobStorageMetadata.Accessibility = Accessibility::Private;
            blobStorageMetadata.Retention = Retention::Temporary;
            blobStorageMetadata.ExpirationDuration = System.TimeSpan::FromMinutes(10080);
        }
        catch(Exception::Error)
        {
            error("@ApplicationPlatform:FileUploadFailed");
        }
        var blobStorageService = new SharedServiceUnitStorage(SharedServiceUnitStorage::GetDefaultStorageContext());
        blobStorageService.UploadData(blobStorageMetadata, DocumentManagement::getAttachmentStream(docuRef));
        SharedServiceUnitStorageData uploadedBlobInfo;
        uploadedBlobInfo = blobStorageService.GetData(blobStorageMetadata.Id, strategy.getAzureStorageCategory(), BlobUrlPermission::Read, System.TimeSpan::FromMinutes(strategy.getBlobLinkExpirationTimeSpanInMinutes()));
        System.IO.Compression.ZipArchive  zipArchive =
            new System.IO.Compression.ZipArchive(File::UseFileFromURL(uploadedBlobInfo.BlobLink), System.IO.Compression.ZipArchiveMode::Update, true);
        
        return zipArchive;
    }

]]></Source>
			</Method>
			<Method>
				<Name>addTelemetry</Name>
				<Source><![CDATA[
    private void addTelemetry(TaxBatchJobGroup _taxBatchJobGroup)
    {
        TaxBatchJobRelation taxBatchJobRelation;

        select count(RecId) from taxBatchJobRelation
            where taxBatchJobRelation.TaxBatchJobGroup == _taxBatchJobGroup.RecId
                && taxBatchJobRelation.IsMergeBatch == NoYes::No;

        int numOfJobs = int642int(taxBatchJobRelation.RecId);

        str eventInfo = new SysInstrumentationEventDetails()
            .add('NumOfJobs', int2Str(numOfJobs))
            .add('StartDateTime', datetime2Str(_taxBatchJobGroup.StartDateTime))
            .add('EndDateTime', datetime2Str(_taxBatchJobGroup.EndDateTime))
            .toJson();

        int64 runtimeInMilliseconds = DateTimeUtil::getDifference(_taxBatchJobGroup.EndDateTime, _taxBatchJobGroup.StartDateTime) * 1000;

        GlobalizationInstrumentationHelper::featureProcessingFinishEvent(
            GlobalizationConstants::FeatureReferenceIN00145,
            eventInfo,
            funcName(),
            '',
            SysCountryRegionCode::countryInfo(),
            runtimeInMilliseconds,
            false,
            true,
            numOfJobs ? runtimeInMilliseconds/numOfJobs : 0); // elapsedTimePerOutputItemInMilliseconds
    }

]]></Source>
			</Method>
			<Method>
				<Name>isRetryable</Name>
				<Source><![CDATA[
    /// <summary>
    /// Controls whether the batch task should be retried in case of transient errors.
    /// </summary>
    /// <returns>
    /// Always returns true.
    /// </returns>
    [Hookable(false)]
    public final boolean isRetryable()
    {
        return true;
    }

]]></Source>
			</Method>
		</Methods>
	</SourceCode>
</AxClass>